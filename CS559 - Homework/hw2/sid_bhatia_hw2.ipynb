{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS559 - Homework #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: September 18th, 2024\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Dr. In Suk Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive Bayes Classification [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to generate the train data set. The code will generate a random data set with four features and classes.\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(n_samples = 400, n_features = 5, centers = 4, cluster_std = 2, random_state = 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [5pts] Compute the prior probability of each class, $p(C_k), \\; \\forall \\, k = 1, ..., 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples = 400, n_features = 5, centers = 4, cluster_std = 2, random_state = 100)\n",
    "\n",
    "unique_classes, counts = np.unique(y, return_counts=True)\n",
    "prior_probabilities = counts / len(y)\n",
    "\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [10pts] Compute the likelihood $p(X \\mid C_k), \\; \\forall \\, 1, ..., 4.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class_means = {}\n",
    "class_covariances = {}\n",
    "class_likelihoods = {}\n",
    "\n",
    "for cls in unique_classes:\n",
    "\n",
    "    X_cls = X[y == cls]\n",
    "    \n",
    "    mean_cls = np.mean(X_cls, axis=0)\n",
    "    class_means[cls] = mean_cls\n",
    "    \n",
    "    cov_cls = np.cov(X_cls, rowvar=False)\n",
    "    class_covariances[cls] = cov_cls\n",
    "    \n",
    "    likelihood_cls = multivariate_normal.pdf(X, mean=mean_cls, cov=cov_cls)\n",
    "    class_likelihoods[cls] = likelihood_cls\n",
    "\n",
    "    # print(f\"\\nClass {cls + 1} Mean Vector:\\n{mean_cls}\")\n",
    "    # print(f\"\\nClass {cls + 1} Covariance Matrix:\\n{cov_cls}\")\n",
    "    print(f\"\\nLikelihoods p(X | C_{cls + 1}):\\n{likelihood_cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [15pts] Compute the posterior probability of each point $p(C_k \\mid X), \\; \\forall \\, k = 1, ..., 4$. Assign the class ID to each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Point 1:\n",
      "True Class: 4\n",
      "p(C_1 | x_1) = 0.0000\n",
      "p(C_2 | x_1) = 0.0000\n",
      "p(C_3 | x_1) = 0.0000\n",
      "p(C_4 | x_1) = 1.0000\n",
      "Assigned Class: 4\n",
      "\n",
      "Data Point 2:\n",
      "True Class: 3\n",
      "p(C_1 | x_2) = 0.0000\n",
      "p(C_2 | x_2) = 0.0000\n",
      "p(C_3 | x_2) = 1.0000\n",
      "p(C_4 | x_2) = 0.0000\n",
      "Assigned Class: 3\n",
      "\n",
      "Data Point 3:\n",
      "True Class: 1\n",
      "p(C_1 | x_3) = 1.0000\n",
      "p(C_2 | x_3) = 0.0000\n",
      "p(C_3 | x_3) = 0.0000\n",
      "p(C_4 | x_3) = 0.0000\n",
      "Assigned Class: 1\n",
      "\n",
      "Data Point 4:\n",
      "True Class: 4\n",
      "p(C_1 | x_4) = 0.0000\n",
      "p(C_2 | x_4) = 0.0000\n",
      "p(C_3 | x_4) = 0.0000\n",
      "p(C_4 | x_4) = 1.0000\n",
      "Assigned Class: 4\n",
      "\n",
      "Data Point 5:\n",
      "True Class: 4\n",
      "p(C_1 | x_5) = 0.0000\n",
      "p(C_2 | x_5) = 0.0000\n",
      "p(C_3 | x_5) = 0.0000\n",
      "p(C_4 | x_5) = 1.0000\n",
      "Assigned Class: 4\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(unique_classes)\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "posterior_probabilities = np.zeros((num_samples, num_classes))\n",
    "for idx, cls in enumerate(unique_classes):\n",
    "    prior = prior_probabilities[cls]\n",
    "    likelihood = class_likelihoods[cls]\n",
    "    posterior_probabilities[:, idx] = likelihood * prior\n",
    "\n",
    "posterior_probabilities_sum = np.sum(posterior_probabilities, axis=1, keepdims=True)\n",
    "posterior_probabilities_normalized = posterior_probabilities / posterior_probabilities_sum\n",
    "\n",
    "predicted_classes = np.argmax(posterior_probabilities_normalized, axis=1)\n",
    "predicted_class_labels = unique_classes[predicted_classes]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nData Point {i + 1}:\")\n",
    "    print(f\"True Class: {y[i] + 1}\")\n",
    "    for idx, cls in enumerate(unique_classes):\n",
    "        print(f\"p(C_{cls + 1} | x_{i + 1}) = {posterior_probabilities_normalized[i, idx]:.4f}\")\n",
    "    print(f\"Assigned Class: {predicted_class_labels[i] + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. [5pts] Construct the confusion matrix to show the classification rate using `sklearn.metrics.confusion_matrix`. The confusion matrix should visualize and summarize the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   0   0   0]\n",
      " [  0 100   0   0]\n",
      " [  0   0 100   0]\n",
      " [  0   0   0 100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y, predicted_class_labels)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. [5pts] Classify the target using `sklearn.native_bayes.GaussianNB`. Report the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X, y)\n",
    "\n",
    "y_pred = gnb.predict(X)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perceptron [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [15pts] In the lecture slide, the methods needed for the perceptron algorithm are provided: `step(X)` and `perceptron_predict(w, X)`. Write a method (`Perceptron_fit(w, X, y, learning_rate, iteration`) that fits the data and returns `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X):\n",
    "    return 1 if X > 0 else -1\n",
    "\n",
    "def perceptron_predict(w, X):\n",
    "    return step(np.dot(np.append(1, X), w))\n",
    "\n",
    "def perceptron_fit(w, X, y, learning_rate, iterations):\n",
    "\n",
    "    X_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    for j in range(iterations):\n",
    "\n",
    "        for k in range(len(y)):\n",
    "\n",
    "            h = step(np.dot(X_bias[k], w))\n",
    "            \n",
    "            if h != y[k]:\n",
    "\n",
    "                error = y[k] - h\n",
    "                \n",
    "                w += learning_rate * error * X_bias[k]\n",
    "                \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [10pts] Create a sample of $X$ in Question 1 whose $y \\in \\{0, 1\\}$. Fit the sample data and find $w$ when the learning rate is 0.001 and the iteration is 1. The learning rate and iteration numbers can be tuned to increase the performance if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.036     , -0.00103401,  0.0073094 ,  0.00041277, -0.01454153,\n",
       "        0.00130611])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary = np.where((y == 0) | (y == 1), 0, 1)\n",
    "\n",
    "w_init_binary = np.zeros(6)\n",
    "\n",
    "learning_rate_binary = 0.001\n",
    "iterations_binary = 1\n",
    "\n",
    "w_trained_binary = perceptron_fit(w_init_binary, X, y_binary, learning_rate_binary, iterations_binary)\n",
    "\n",
    "w_trained_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [5pts] Use the final $w$ from 2(b) to classify all observations in $X$. Measure the performance and explain the success. Discuss what other tests can be done in order to improve the classification for $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5525\n"
     ]
    }
   ],
   "source": [
    "def classify(X, w):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        prediction = perceptron_predict(w, X[i])\n",
    "        predictions.append(prediction)\n",
    "    return np.array(predictions)\n",
    "\n",
    "y_pred_binary = classify(X, w_trained_binary)\n",
    "\n",
    "y_pred_binary = np.where(y_pred_binary > 0, 1, 0)\n",
    "\n",
    "accuracy_binary = accuracy_score(y_binary, y_pred_binary)\n",
    "\n",
    "print(accuracy_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an accuracy of around 55.25%, suggesting limited success. To improve performance, several steps can be taken: increasing the number of iterations to allow better convergence, tuning the learning rate for more effective weight updates, and applying feature scaling (normalization or standardization) to handle feature magnitude differences. Additionally, considering non-linear models such as Support Vector Machines (SVM) or neural networks could help capture more complex relationships. Finally, using cross-validation to fine-tune hyperparameters can further optimize the modelâ€™s performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
