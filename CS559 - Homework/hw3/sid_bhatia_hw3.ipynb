{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS559 - Homework #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: October 1st, 2024\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Dr. In Suk Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Networks [60 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data point $\\mathbf{x} = [0.7, 0.1, 0.3, 0.5]$ and $y = 1.5$. In this experiment, we will implement a simple neural network algorithm. In the lecture, the data point was used to observe the computational process of a neural network with a single hidden layer consisting of two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [5pts] Perform a forward propagation when the network with the one hidden layer of two neurons where $ \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.16 & 0.02 & 0.63 & 0.36 \\\\ 0.16 & 0.25 & 0.22 & 0.29 \\end{bmatrix} $ and $ \\mathbf{W}^{(2)} = \\begin{bmatrix} 0.05 \\\\ 0.33 \\end{bmatrix} $. The predicted value $\\hat{h}$ should be $0.139$ if the learning rate $\\eta = 0.1$ is used. However, it can be changed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value hat_h = 0.1390\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.7, 0.1, 0.3, 0.5])\n",
    "\n",
    "W1 = np.array([\n",
    "    [0.16, 0.02, 0.63, 0.36],\n",
    "    [0.16, 0.25, 0.22, 0.29]\n",
    "])\n",
    "\n",
    "W2 = np.array([0.05, 0.33])\n",
    "\n",
    "z1 = np.dot(W1, x)   \n",
    "a1 = z1              \n",
    "\n",
    "z2 = np.dot(W2, a1)  \n",
    "hat_h = z2           \n",
    "\n",
    "print(f\"Predicted value hat_h = {hat_h:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [5 pts] Perform the backpropagation. Report the updated $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated W1:\n",
      "[[0.16476354 0.02068051 0.63204152 0.36340252]\n",
      " [0.19143933 0.25449133 0.233474   0.31245666]]\n",
      "\n",
      "Updated W2:\n",
      "[0.11573678 0.37736315]\n"
     ]
    }
   ],
   "source": [
    "y = 1.5\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "delta_output = hat_h - y\n",
    "\n",
    "dE_dW2 = delta_output * a1\n",
    "\n",
    "delta_hidden = delta_output * W2\n",
    "\n",
    "dE_dW1 = np.outer(delta_hidden, x)\n",
    "\n",
    "W2_new = W2 - eta * dE_dW2\n",
    "W1_new = W1 - eta * dE_dW1\n",
    "\n",
    "print(\"\\nUpdated W1:\")\n",
    "print(W1_new)\n",
    "\n",
    "print(\"\\nUpdated W2:\")\n",
    "print(W2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [10pts] Repeat the forward and backpropagation to optimize $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$. Predict $y$ using the optimized $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized predicted value hat_h = 1.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    z1 = np.dot(W1, x)   \n",
    "    a1 = z1              \n",
    "\n",
    "    z2 = np.dot(W2, a1)  \n",
    "    hat_h = z2           \n",
    "\n",
    "    loss = 0.5 * (hat_h - y) ** 2\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    delta_output = hat_h - y\n",
    "\n",
    "    dE_dW2 = delta_output * a1  \n",
    "\n",
    "    delta_hidden = delta_output * W2 \n",
    "\n",
    "    dE_dW1 = np.outer(delta_hidden, x)  \n",
    "   \n",
    "    W2 -= eta * dE_dW2\n",
    "    W1 -= eta * dE_dW1\n",
    "\n",
    "z1_opt = np.dot(W1, x)\n",
    "a1_opt = z1_opt\n",
    "\n",
    "z2_opt = np.dot(W2, a1_opt)\n",
    "hat_h_opt = z2_opt\n",
    "\n",
    "print(f\"\\nOptimized predicted value hat_h = {hat_h_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. [10pts] Consider the same network as (c) except that a non-linear activation is applied. Derive analytical solution of the error matrices, $\\bm{\\delta}^{(2)}$ and $\\bm{\\delta}^{(1)}$, when a sigmoid function is applied as a non-linear activation function between the input and hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. [15pts] Predict $y$ for (d). Visualize the convergence of error and compare it to the result in (c). Explain which one converged faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
