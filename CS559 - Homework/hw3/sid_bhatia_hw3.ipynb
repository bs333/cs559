{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS559 - Homework #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: October 1st, 2024\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Dr. In Suk Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Networks [60 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data point $\\mathbf{x} = [0.7, 0.1, 0.3, 0.5]$ and $y = 1.5$. In this experiment, we will implement a simple neural network algorithm. In the lecture, the data point was used to observe the computational process of a neural network with a single hidden layer consisting of two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [5pts] Perform a forward propagation when the network with the one hidden layer of two neurons where $ \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.16 & 0.02 & 0.63 & 0.36 \\\\ 0.16 & 0.25 & 0.22 & 0.29 \\end{bmatrix} $ and $ \\mathbf{W}^{(2)} = \\begin{bmatrix} 0.05 \\\\ 0.33 \\end{bmatrix} $. The predicted value $\\hat{h}$ should be $0.139$ if the learning rate $\\eta = 0.1$ is used. However, it can be changed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value h_hat: 0.139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.7, 0.1, 0.3, 0.5])\n",
    "\n",
    "W1 = np.array([\n",
    "    [0.16, 0.02, 0.63, 0.36],\n",
    "    [0.16, 0.25, 0.22, 0.29]\n",
    "])\n",
    "\n",
    "W2 = np.array([\n",
    "    [0.05],\n",
    "    [0.33]\n",
    "])\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "z1 = W1.dot(x)\n",
    "\n",
    "a1 = relu(z1)\n",
    "\n",
    "z2 = W2.T.dot(a1)\n",
    "\n",
    "predicted_value = z2[0]\n",
    "\n",
    "print(f\"Predicted value h_hat: {predicted_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [5 pts] Perform the backpropagation. Report the updated $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated W^(1):\n",
      "[[0.16476354 0.02068051 0.63204152 0.36340252]\n",
      " [0.19143933 0.25449133 0.233474   0.31245666]]\n",
      "\n",
      "Updated W^(2):\n",
      "[[0.11573678]\n",
      " [0.37736315]]\n"
     ]
    }
   ],
   "source": [
    "y = 1.5\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "loss = 0.5 * (predicted_value - y) ** 2\n",
    "\n",
    "delta_output = predicted_value - y\n",
    "\n",
    "gradient_W2 = delta_output * a1.reshape(-1, 1)\n",
    "\n",
    "delta_hidden = (W2.flatten() * delta_output) * relu_derivative(z1)\n",
    "\n",
    "gradient_W1 = delta_hidden.reshape(-1, 1) * x.reshape(1, -1)\n",
    "\n",
    "W2_updated = W2 - eta * gradient_W2\n",
    "W1_updated = W1 - eta * gradient_W1\n",
    "\n",
    "print(\"Updated W^(1):\")\n",
    "print(W1_updated)\n",
    "print(\"\\nUpdated W^(2):\")\n",
    "print(W2_updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
