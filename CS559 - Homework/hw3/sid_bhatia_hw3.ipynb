{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS559 - Homework #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: October 1st, 2024\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Dr. In Suk Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Networks [60 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data point $\\mathbf{x} = [0.7, 0.1, 0.3, 0.5]$ and $y = 1.5$. In this experiment, we will implement a simple neural network algorithm. In the lecture, the data point was used to observe the computational process of a neural network with a single hidden layer consisting of two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [5pts] Perform a forward propagation when the network with the one hidden layer of two neurons where $ \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.16 & 0.02 & 0.63 & 0.36 \\\\ 0.16 & 0.25 & 0.22 & 0.29 \\end{bmatrix} $ and $ \\mathbf{W}^{(2)} = \\begin{bmatrix} 0.05 \\\\ 0.33 \\end{bmatrix} $. The predicted value $\\hat{h}$ should be $0.139$ if the learning rate $\\eta = 0.1$ is used. However, it can be changed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value hat_h = 0.1390\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.7, 0.1, 0.3, 0.5])\n",
    "\n",
    "W1 = np.array([\n",
    "    [0.16, 0.02, 0.63, 0.36],\n",
    "    [0.16, 0.25, 0.22, 0.29]\n",
    "])\n",
    "\n",
    "W2 = np.array([0.05, 0.33])\n",
    "\n",
    "z1 = np.dot(W1, x)   \n",
    "a1 = z1              \n",
    "\n",
    "z2 = np.dot(W2, a1)  \n",
    "hat_h = z2           \n",
    "\n",
    "print(f\"Predicted value hat_h = {hat_h:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [5 pts] Perform the backpropagation. Report the updated $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated W1:\n",
      "[[0.16476354 0.02068051 0.63204152 0.36340252]\n",
      " [0.19143933 0.25449133 0.233474   0.31245666]]\n",
      "\n",
      "Updated W2:\n",
      "[0.11573678 0.37736315]\n"
     ]
    }
   ],
   "source": [
    "y = 1.5\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "delta_output = hat_h - y\n",
    "\n",
    "dE_dW2 = delta_output * a1\n",
    "\n",
    "delta_hidden = delta_output * W2\n",
    "\n",
    "dE_dW1 = np.outer(delta_hidden, x)\n",
    "\n",
    "W2_new = W2 - eta * dE_dW2\n",
    "W1_new = W1 - eta * dE_dW1\n",
    "\n",
    "print(\"\\nUpdated W1:\")\n",
    "print(W1_new)\n",
    "\n",
    "print(\"\\nUpdated W2:\")\n",
    "print(W2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [10pts] Repeat the forward and backpropagation to optimize $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$. Predict $y$ using the optimized $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized predicted value hat_h = 1.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    z1 = np.dot(W1, x)   \n",
    "    a1 = z1              \n",
    "\n",
    "    z2 = np.dot(W2, a1)  \n",
    "    hat_h = z2           \n",
    "\n",
    "    loss = 0.5 * (hat_h - y) ** 2\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    delta_output = hat_h - y\n",
    "\n",
    "    dE_dW2 = delta_output * a1  \n",
    "\n",
    "    delta_hidden = delta_output * W2 \n",
    "\n",
    "    dE_dW1 = np.outer(delta_hidden, x)  \n",
    "   \n",
    "    W2 -= eta * dE_dW2\n",
    "    W1 -= eta * dE_dW1\n",
    "\n",
    "z1_opt = np.dot(W1, x)\n",
    "a1_opt = z1_opt\n",
    "\n",
    "z2_opt = np.dot(W2, a1_opt)\n",
    "hat_h_opt = z2_opt\n",
    "\n",
    "print(f\"\\nOptimized predicted value hat_h = {hat_h_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. [10pts] Consider the same network as (c) except that a non-linear activation is applied. Derive analytical solution of the error matrices, $\\bm{\\delta}^{(2)}$ and $\\bm{\\delta}^{(1)}$, when a sigmoid function is applied as a non-linear activation function between the input and hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the error matrices $\\bm{\\delta}^{(2)}$ and $\\bm{\\delta}^{(1)}$ when a sigmoid activation function is applied between the input and hidden layers, we follow the standard backpropagation procedure for neural networks with non-linear activations.\n",
    "\n",
    "##### Forward Propagation\n",
    "\n",
    "Compute the input to the hidden layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Apply the sigmoid activation function to obtain the hidden layer activations:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) = \\frac{1}{1 + e^{-\\mathbf{z}^{(1)}}}\n",
    "$$\n",
    "\n",
    "Compute the input to the output layer (assuming linear activation at the output):\n",
    "\n",
    "$$\n",
    "z^{(2)} = \\mathbf{W}^{(2)} \\mathbf{a}^{(1)}\n",
    "$$\n",
    "\n",
    "Compute the output activation (since activation is linear):\n",
    "\n",
    "$$\n",
    "a^{(2)} = z^{(2)}\n",
    "$$\n",
    "\n",
    "##### Backpropagation\n",
    "\n",
    "Compute the error term at the output layer:\n",
    "\n",
    "The derivative of the loss with respect to $z^{(2)}$ is:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial E}{\\partial z^{(2)}} = \\frac{\\partial E}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\n",
    "$$\n",
    "\n",
    "Since $E = \\dfrac{1}{2} (a^{(2)} - y)^2$ and $a^{(2)} = z^{(2)}$, we have:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial a^{(2)}} = a^{(2)} - y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = 1\n",
    "$$\n",
    "  \n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = a^{(2)} - y\n",
    "$$\n",
    "\n",
    "Compute the error term at the hidden layer:\n",
    "    \n",
    "The error term for the hidden layer neurons is:\n",
    "$$\n",
    "\\bm{\\delta}^{(1)} = \\left( \\mathbf{W}^{(2)} \\right)^\\top \\delta^{(2)} \\odot \\sigma'\\left( \\mathbf{z}^{(1)} \\right)\n",
    "$$\n",
    "   \n",
    "Here, $\\odot$ denotes element-wise multiplication, and $\\sigma'\\left( \\mathbf{z}^{(1)} \\right)$ is the derivative of the sigmoid function evaluated at $\\mathbf{z}^{(1)}$.\n",
    "    \n",
    "Compute the derivative of the sigmoid activation function:\n",
    "    \n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z) \\left( 1 - \\sigma(z) \\right)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\sigma'\\left( \\mathbf{z}^{(1)} \\right) = \\mathbf{a}^{(1)} \\odot \\left( 1 - \\mathbf{a}^{(1)} \\right)\n",
    "$$\n",
    "    \n",
    "Substitute back into $\\bm{\\delta}^{(1)}$:\n",
    "\n",
    "$$\n",
    "\\bm{\\delta}^{(1)} = \\left( \\mathbf{W}^{(2)} \\right)^\\top \\delta^{(2)} \\odot \\left( \\mathbf{a}^{(1)} \\odot \\left( 1 - \\mathbf{a}^{(1)} \\right) \\right)\n",
    "$$\n",
    "\n",
    "##### Summary of the Error Matrices\n",
    "\n",
    "Output Layer Error Term:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = a^{(2)} - y\n",
    "$$\n",
    "\n",
    "Hidden Layer Error Term:\n",
    "$$\n",
    "\\bm{\\delta}^{(1)} = \\left( \\mathbf{W}^{(2)} \\right)^\\top \\delta^{(2)} \\odot \\left( \\mathbf{a}^{(1)} \\odot \\left( 1 - \\mathbf{a}^{(1)} \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. [15pts] Predict $y$ for (d). Visualize the convergence of error and compare it to the result in (c). Explain which one converged faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
